name: CreateCluster
description: 'Installs Go Downloads and installs Karpenter Dependencies'
inputs:
  account_id:
    description: "Account ID to access AWS"
    required: true
  role:
    description: "Role to access AWS"
    required: true
  region:
    description: "Region to access AWS"
    required: true
  cluster_name:
    description: 'Name of the cluster to be launched by eksctl'
    required: true
  k8s_version:
    description: 'Version of Kubernetes to use for the launched cluster'
    default: "1.28"
  eksctl_version:
    description: "Version of eksctl to install"
    default: v0.164.0
  ip_family:
    description: "IP Family of the cluster. Valid values are IPv4 or IPv6"
    default: "IPv4"
  private_cluster:
    description: "Whether to create a private cluster which does not add access to the public internet. Valid values are 'true' or 'false'"
    default: 'false'
  git_ref:
    description: "The git commit, tag, or branch to check out. Requires a corresponding Karpenter snapshot release"
    required: false
runs:
  using: "composite"
  steps:
  - uses: actions/checkout@v4
    with:
      ref: ${{ inputs.git_ref }}
  - uses: ./.github/actions/e2e/install-eksctl
    with:
      version: ${{ inputs.eksctl_version }}
  - name: create iam policies
    shell: bash
    run: |
      # Resolve the cloudformation path with fallback
      CLOUDFORMATION_PATH=website/content/en/preview/getting-started/getting-started-with-karpenter/cloudformation.yaml
      if [ ! -f $CLOUDFORMATION_PATH ]; then
        CLOUDFORMATION_PATH=website/content/en/preview/getting-started/getting-started-with-eksctl/cloudformation.yaml
      fi

      # Update the Cloudformation policy to add the permissionBoundary to the NodeRole
      yq -i '.Resources.KarpenterNodeRole.Properties.PermissionsBoundary="arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"' $CLOUDFORMATION_PATH

      aws iam create-service-linked-role --aws-service-name spot.amazonaws.com || true
      aws cloudformation deploy \
        --stack-name iam-${{ inputs.cluster_name }} \
        --template-file $CLOUDFORMATION_PATH \
        --capabilities CAPABILITY_NAMED_IAM \
        --parameter-overrides "ClusterName=${{ inputs.cluster_name }}" \
        --tags "testing/type=e2e" "testing/cluster=${{ inputs.cluster_name }}" "github.com/run-url=https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" "karpenter.sh/discovery=${{ inputs.cluster_name }}"
  - name: create or upgrade cluster
    shell: bash
    run: |
      # Create or Upgrade the cluster based on whether the cluster already exists
      cmd="create"
      eksctl get cluster --name ${{ inputs.cluster_name }} && cmd="upgrade"

      cat << EOF >> clusterconfig.yaml
      ---
      apiVersion: eksctl.io/v1alpha5
      kind: ClusterConfig
      metadata:
        name: ${{ inputs.cluster_name }}
        region: ${{ inputs.region }}
        version: "${{ inputs.k8s_version }}"
        tags:
          karpenter.sh/discovery: "${{ inputs.cluster_name }}"
          github.com/run-url: "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          testing/type: "e2e"
          testing/cluster: "${{ inputs.cluster_name }}"
      kubernetesNetworkConfig:
        ipFamily: ${{ inputs.ip_family }}
      managedNodeGroups:
        - instanceType: c5.4xlarge
          amiFamily: AmazonLinux2
          name: ${{ inputs.cluster_name }}-system-pool
          desiredCapacity: 2
          disableIMDSv1: true
          minSize: 2
          maxSize: 2
          iam:
            instanceRolePermissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
          taints:
          - key: CriticalAddonsOnly
            value: "true"
            effect: NoSchedule
      cloudWatch:
        clusterLogging:
          enableTypes: ["*"]
          logRetentionInDays: 30
      iam:
        serviceRolePermissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
        serviceAccounts:
          - metadata:
              name: karpenter
              namespace: karpenter
            attachPolicyARNs:
              - "arn:aws:iam::${{ inputs.account_id }}:policy/KarpenterControllerPolicy-${{ inputs.cluster_name }}"
            permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
            roleName: karpenter-irsa-${{ inputs.cluster_name }}
            roleOnly: true
          - metadata:
              name: prometheus-kube-prometheus-prometheus
              namespace: prometheus
            attachPolicyARNs:
              - "arn:aws:iam::${{ inputs.account_id }}:policy/PrometheusWorkspaceIngestionPolicy"
            permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
            roleName: prometheus-irsa-${{ inputs.cluster_name }}
            roleOnly: true
        withOIDC: true
      addons:
      - name: vpc-cni
        permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
      - name: coredns
        permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
      - name: kube-proxy
        permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
      - name: aws-ebs-csi-driver
        permissionsBoundary: "arn:aws:iam::${{ inputs.account_id }}:policy/GithubActionsPermissionsBoundary"
        wellKnownPolicies:
          ebsCSIController: true
      EOF
      
      if [[ ${{ inputs.private_cluster }} == 'true' ]]; then
        yq -i '.privateCluster.enabled=true' clusterconfig.yaml
        yq -i '.managedNodeGroups[0].privateNetworking=true' clusterconfig.yaml
      fi

      eksctl ${cmd} cluster -f clusterconfig.yaml

      # We need to call these update iamserviceaccount commands again since the "eksctl upgrade cluster" action
      # doesn't handle updates to IAM serviceaccounts correctly when the roles assigned to them change
      eksctl update iamserviceaccount -f clusterconfig.yaml --approve
      
      # Add the SQS and SSM VPC endpoints if we are creating a private cluster
      # We need to grab all of the VPC details for the cluster in order to add the endpoint
      if [[ ${{ inputs.private_cluster }} == 'true' ]]; then
        VPC_CONFIG=$(aws eks describe-cluster --name ${{ inputs.cluster_name }} --query "cluster.resourcesVpcConfig")
        VPC_ID=$(echo $VPC_CONFIG | jq .vpcId -r)
        SUBNET_IDS=($(echo $VPC_CONFIG | jq '.subnetIds | join(" ")' -r))
        SECURITY_GROUP_IDS=($(echo $VPC_CONFIG | jq '.securityGroupIds | join(" ")' -r))
        
        for SERVICE in "com.amazonaws.${{ inputs.region }}.ssm" "com.amazonaws.${{ inputs.region }}.sqs"; do
          aws ec2 create-vpc-endpoint \
            --vpc-id "${VPC_ID}" \
            --vpc-endpoint-type Interface \
            --service-name "${SERVICE}" \
            --subnet-ids ${SUBNET_IDS[@]} \
            --security-group-ids ${SECURITY_GROUP_IDS[@]} \
            --tags "testing/type=e2e" "testing/cluster=${{ inputs.cluster_name }}" "github.com/run-url=https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" "karpenter.sh/discovery=${{ inputs.cluster_name }}"
        done
      fi
  - name: tag oidc provider of the cluster
    if: always()
    shell: bash
    run: |
      oidc_id=$(aws eks describe-cluster --name ${{ inputs.cluster_name }} --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 3,4,5)
      arn="arn:aws:iam::${{ inputs.account_id }}:oidc-provider/${oidc_id}"
      aws iam tag-open-id-connect-provider --open-id-connect-provider-arn $arn \
         --tags Key=testing/type,Value=e2e Key=testing/cluster,Value=${{ inputs.cluster_name }} Key=github.com/run-url,Value=https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
  - name: give KarpenterNodeRole permission to bootstrap
    shell: bash
    run: |
      eksctl create iamidentitymapping \
      --username system:node:{{EC2PrivateDNSName}} \
      --cluster "${{ inputs.cluster_name }}" \
      --arn "arn:aws:iam::${{ inputs.account_id }}:role/KarpenterNodeRole-${{ inputs.cluster_name }}" \
      --group system:bootstrappers \
      --group system:nodes
  - name: cloudformation describe stack events
    shell: bash
    if: failure()
    run: |
      stack_names=$(aws cloudformation describe-stacks --query 'Stacks[?Tags[?Key == `karpenter.sh/discovery` && Value == `${{ inputs.CLUSTER_NAME }}`]].{StackName: StackName}' --output text)
      for stack_name in $stack_names; do
        echo "Stack Events for $stack_name:"
        aws cloudformation describe-stack-events --stack-name $stack_name
      done
